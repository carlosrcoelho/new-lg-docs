# Buffer Memory

The `Conversation Buffer Memory` stores messages and extracts them into variables.

When you open the chat and start a conversation, it will take the message as input and return a response. LLM generates the response, and the conversation message will be store in the Buffer Memory. 

Learn more about the Buffer Memory [here](https://python.langchain.com/en/latest/modules/memory/types/buffer.html).

## ⛓️LangFlow example

import ThemedImage from '@theme/ThemedImage';
import useBaseUrl from '@docusaurus/useBaseUrl';
import ZoomableImage from '/src/theme/ZoomableImage.js';

<ZoomableImage
  alt="Docusaurus themed image"
  sources={{
    light: 'img/buffer-memory.png',
  }}
/>

 ### <a target="\_blank" href="json_files/Buffer_Memory.json" download>Download Flow</a>

  **Components used in this flow:**

The LLM in this example is [`ChatOpenAI`](https://platform.openai.com/), and the chain is a `Conversation Chain`.